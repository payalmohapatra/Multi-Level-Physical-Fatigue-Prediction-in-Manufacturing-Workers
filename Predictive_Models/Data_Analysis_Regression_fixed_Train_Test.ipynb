{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Assymmetric loss function\n",
    "\n",
    "1. Feature extraction --> windowing(3 mins), handcrafted features, \n",
    "2. Data split\n",
    "3. Conduct regression with different loss functions\n",
    "4. Scoring methods :\n",
    "    MAE\n",
    "    MBR\n",
    "    MSE\n",
    "5. Get results for all composite, ziptie and combined\n",
    "6. Include Gender as the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "## Scikit related\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import signal\n",
    "from scipy import integrate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm, kurtosis\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Updated helper fucntion made for the new data from formal user study\n",
    "\n",
    "from helper_functions_user_study_2_0 import get_all_file_paths\n",
    "from helper_functions_user_study_2_0 import regr_feature_extract_one_segment\n",
    "from helper_functions_user_study_2_0 import feature_scaling\n",
    "from helper_functions_user_study_2_0 import regr_segment_time_series\n",
    "# from helper_functions_user_study import SVR_linear\n",
    "# from helper_functions_user_study import SVR_poly\n",
    "# from helper_functions_user_study import SVR_rbf\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataframe for Regression Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __read_csv_drop_unnamed__(csv_path) :\n",
    "    pd_test = pd.read_csv(csv_path)\n",
    "    pd_test.columns.str.match(\"Unnamed\")\n",
    "    pd_test = pd_test.loc[:,~pd_test.columns.str.match(\"Unnamed\")]\n",
    "    return pd_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_segment_df (segment) :\n",
    "    df = pd.DataFrame(segment, columns=[\n",
    "        ['Time',\n",
    " 'HR_Processed',\n",
    " 'HR',\n",
    " 'HRV',\n",
    " 'RR',\n",
    " 'RRSQI',\n",
    " 'ECGSQI',\n",
    " 'ECG',\n",
    " 'acc_X',\n",
    " 'acc_Y',\n",
    " 'acc_Z',\n",
    " 'Temperature',\n",
    " 'IMU_1_ax_g_',\n",
    " 'IMU_1_ay_g_',\n",
    " 'IMU_1_az_g_',\n",
    " 'IMU_1_gx_dps_',\n",
    " 'IMU_1_gy_dps_',\n",
    " 'IMU_1_gz_dps_',\n",
    " 'IMU_2_ax_g_',\n",
    " 'IMU_2_ay_g_',\n",
    " 'IMU_2_az_g_',\n",
    " 'IMU_2_gx_dps_',\n",
    " 'IMU_2_gy_dps_',\n",
    " 'IMU_2_gz_dps_',\n",
    " 'IMU_3_ax_g_',\n",
    " 'IMU_3_ay_g_',\n",
    " 'IMU_3_az_g_',\n",
    " 'IMU_3_gx_dps_',\n",
    " 'IMU_3_gy_dps_',\n",
    " 'IMU_3_gz_dps_',\n",
    " 'IMU_4_ax_g_',\n",
    " 'IMU_4_ay_g_',\n",
    " 'IMU_4_az_g_',\n",
    " 'IMU_4_gx_dps_',\n",
    " 'IMU_4_gy_dps_',\n",
    " 'IMU_4_gz_dps_',\n",
    " 'IMU_5_ax_g_',\n",
    " 'IMU_5_ay_g_',\n",
    " 'IMU_5_az_g_',\n",
    " 'IMU_5_gx_dps_',\n",
    " 'IMU_5_gy_dps_',\n",
    " 'IMU_5_gz_dps_',\n",
    " 'Physical Fatigue - Initial',\n",
    " 'Physical Fatigue - Final',\n",
    " 'Mental Fatigue - Initial',\n",
    " 'Mental Fatigue - Final',\n",
    " 'Performance rating',\n",
    " 'Age',\n",
    " 'Weight',\n",
    " 'Height',\n",
    " 'Weights added',\n",
    " 'Gender']\n",
    "    ])\n",
    "    return df  \n",
    "def file_segment_features(test_seg) :\n",
    "#    print('Inside segment features') \n",
    "   # Give segmented 3D array to this --> Features dataframe\n",
    "   test_one_seg = conv_segment_df(test_seg[0])\n",
    "   features_df = regr_feature_extract_one_segment(test_one_seg)\n",
    "   #features_df.head()\n",
    "   for i in range(1, np.shape(test_seg)[0]) :\n",
    "    # print('===============================================================I am in segment ', i)\n",
    "    temp_seg = conv_segment_df(test_seg[i])\n",
    "    df = regr_feature_extract_one_segment(temp_seg)\n",
    "    features_df = features_df.append(df)\n",
    "    \n",
    "   return features_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15832\\2500014087.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;31m# Get all Composite Files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m360\u001b[0m \u001b[1;31m## 120 is one min\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0msource_data_path_composite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'MxD_Data_User_Study/composite/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 'composite_train/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15832\\824777655.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(test_seg)\u001b[0m\n\u001b[0;32m     63\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# print('===============================================================I am in segment ', i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mtemp_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_segment_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregr_feature_extract_one_segment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_seg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mfeatures_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mohap\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Get all Composite Files\n",
    "window_size = 360 ## 120 is one min\n",
    "source_data_path_composite = 'MxD_Data_User_Study/composite/'\n",
    "# 'composite_train/'\n",
    "source_data_path_ziptie = 'MxD_Data_User_Study/ziptie/'\n",
    "# 'ziptie_train/'\n",
    "\n",
    "\n",
    "# this function is to get all the interpolated and segmented data\n",
    "folder_list_composite = os.listdir(source_data_path_composite)\n",
    "composite_file_names = get_all_file_paths(source_data_path_composite, folder_list_composite)\n",
    "\n",
    "composite_main_dataframe = pd.DataFrame(__read_csv_drop_unnamed__(composite_file_names[0]))\n",
    "test_seg_0 = regr_segment_time_series(window_size, composite_main_dataframe)\n",
    "composite_main_dataframe = file_segment_features(test_seg_0)\n",
    "for i in range(1,len(composite_file_names)):\n",
    "  data = __read_csv_drop_unnamed__(composite_file_names[i])\n",
    "  if (len(data) < window_size) :\n",
    "    pass\n",
    "  else :\n",
    "    test_seg = regr_segment_time_series(window_size,data)\n",
    "    features_df = file_segment_features(test_seg)\n",
    "    composite_main_dataframe = composite_main_dataframe.append(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to get all the interpolated and segmented data\n",
    "folder_list_ziptie = os.listdir(source_data_path_ziptie)\n",
    "ziptie_file_names = get_all_file_paths(source_data_path_ziptie, folder_list_ziptie)\n",
    "\n",
    "ziptie_main_dataframe = pd.DataFrame(__read_csv_drop_unnamed__(ziptie_file_names[0]))\n",
    "test_seg_0 = regr_segment_time_series(window_size, ziptie_main_dataframe)\n",
    "ziptie_main_dataframe = file_segment_features(test_seg_0)\n",
    "for i in range(1,len(ziptie_file_names)):\n",
    "  data = __read_csv_drop_unnamed__(ziptie_file_names[i])\n",
    "  if (len(data) < window_size) :\n",
    "    pass\n",
    "  else :\n",
    "    test_seg = regr_segment_time_series(window_size,data)\n",
    "    features_df = file_segment_features(test_seg)\n",
    "    ziptie_main_dataframe = ziptie_main_dataframe.append(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rows with 0 HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_main_dataframe = composite_main_dataframe.loc[~((composite_main_dataframe['max_hr'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziptie_main_dataframe = ziptie_main_dataframe.loc[~((ziptie_main_dataframe['max_hr'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataframe = ziptie_main_dataframe\n",
    "combined_dataframe = combined_dataframe.append(composite_main_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the gyro features\n",
    "composite_main_dataframe = composite_main_dataframe.drop(['range_gyro_imu1', 'rms_imu1_gyro',\n",
    "                                                          'range_gyro_imu2', 'rms_imu2_gyro',\n",
    "                                                          'range_gyro_imu3', 'rms_imu3_gyro',\n",
    "                                                          'range_gyro_imu4', 'rms_imu4_gyro',\n",
    "                                                           'rms_imu5_gyro'], axis='columns')\n",
    "ziptie_main_dataframe = ziptie_main_dataframe.drop(['range_gyro_imu1', 'rms_imu1_gyro',\n",
    "                                                          'range_gyro_imu2', 'rms_imu2_gyro',\n",
    "                                                          'range_gyro_imu3', 'rms_imu3_gyro',\n",
    "                                                          'range_gyro_imu4', 'rms_imu4_gyro',\n",
    "                                                           'rms_imu5_gyro'], axis='columns')                                                           \n",
    "combined_dataframe = combined_dataframe.drop(['range_gyro_imu1', 'rms_imu1_gyro',\n",
    "                                                          'range_gyro_imu2', 'rms_imu2_gyro',\n",
    "                                                          'range_gyro_imu3', 'rms_imu3_gyro',\n",
    "                                                          'range_gyro_imu4', 'rms_imu4_gyro',\n",
    "                                                           'rms_imu5_gyro'], axis='columns') \n",
    "\n",
    "\n",
    "# composite_main_dataframe_test = composite_main_dataframe_test.drop(['range_gyro_imu1', 'rms_imu1_gyro',\n",
    "#                                                           'range_gyro_imu2', 'rms_imu2_gyro',\n",
    "#                                                           'range_gyro_imu3', 'rms_imu3_gyro',\n",
    "#                                                           'range_gyro_imu4', 'rms_imu4_gyro',\n",
    "#                                                            'rms_imu5_gyro'], axis='columns')\n",
    "# ziptie_main_dataframe_test = ziptie_main_dataframe_test.drop(['range_gyro_imu1', 'rms_imu1_gyro',\n",
    "#                                                           'range_gyro_imu2', 'rms_imu2_gyro',\n",
    "#                                                           'range_gyro_imu3', 'rms_imu3_gyro',\n",
    "#                                                           'range_gyro_imu4', 'rms_imu4_gyro',\n",
    "#                                                            'rms_imu5_gyro'], axis='columns')                                                           \n",
    "# combined_dataframe_test = combined_dataframe_test.drop(['range_gyro_imu1', 'rms_imu1_gyro',\n",
    "#                                                           'range_gyro_imu2', 'rms_imu2_gyro',\n",
    "#                                                           'range_gyro_imu3', 'rms_imu3_gyro',\n",
    "#                                                           'range_gyro_imu4', 'rms_imu4_gyro',\n",
    "#                                                            'rms_imu5_gyro'], axis='columns') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the dataset\n",
    "df_scaled_combined = feature_scaling(0,10, combined_dataframe)\n",
    "df_scaled_composite = feature_scaling(0,10, composite_main_dataframe)\n",
    "df_scaled_ziptie = feature_scaling(0,10, ziptie_main_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziptie_main_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training only Composite Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop initial fatigue for training\n",
    "\"\"\" July 11th :Does it make sense to even include the initial fatigue anymore? --> It would be like including the time delayed version of our target itself.\n",
    "First trial without any initial fatigue. Include initial fatigue after you increase the window size.\n",
    "\"\"\"\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(\n",
    "    df_scaled_composite.drop(['final_fatigue', 'init_fatigue'], axis='columns'), composite_main_dataframe['final_fatigue'], test_size=0.3)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "Y_train = Y_train.to_numpy()\n",
    "\n",
    "X_test = X_test.to_numpy()\n",
    "Y_test = Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assymetric losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.linspace(-1,1, 1000)\n",
    "LINEX_loss = lambda E, a: (2/a**2)*(np.exp(a*E) - a*E - 1)\n",
    "plt.plot(E, LINEX_loss(E, a= -1))\n",
    "plt.xlabel('Error (Y_pred-Y_actual)')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSYM_loss = lambda E : np.where((E)<0, 2*(E**2), (E**2))\n",
    "plt.plot(E, ASSYM_loss(E))\n",
    "plt.xlabel('Error (Y_pred-Y_actual)')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __compute_mae__(y_pred, y_test) :\n",
    "    ## traslatet to an array\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = np.array(y_test)\n",
    "    error = np.zeros(len(y_test))\n",
    "    error = np.array(error)\n",
    "    neg_error = 0\n",
    "    pos_error = 0\n",
    "    mae = np.mean(np.abs(y_pred - y_test))\n",
    "    print('MAE = ', mae)\n",
    "\n",
    "\n",
    "def __compute_rel_predictions__(y_pred, y_test) :\n",
    "    ## traslatet to an array\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = np.array(y_test)\n",
    "    error = np.zeros(len(y_test))\n",
    "    error = np.array(error)\n",
    "    neg_error = 0\n",
    "    pos_error = 0\n",
    "    for i in range(0, len(y_test)) :\n",
    "        error[i] = y_pred[i] - y_test[i]\n",
    "        if error[i] < 0 :\n",
    "            neg_error += 1 ## Under prediction --> more dangerous\n",
    "        elif error[i] > 0 :\n",
    "            pos_error += 1 ## Over prediction\n",
    "    print('Underpredictions :', neg_error/len(y_test)) \n",
    "    print('Overpredictions :', pos_error/len(y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sources : \n",
    "1. https://arxiv.org/pdf/1603.02754.pdf \n",
    "2. https://github.com/froukje/articles/blob/main/02_custom_loss_xgboost.ipynb\n",
    "3. https://www.datatrigger.org/post/asymmetric_loss/ \n",
    "\"\"\"\n",
    "# def mean_bias_error(y_pred, y_true) :\n",
    "\n",
    "def xgb_model(X_train, y_train,X_val, y_val, \n",
    "              objective='reg:squarederror',\n",
    "              learning_rate=0.3,\n",
    "              min_child_weight=1,\n",
    "              lambda_=1,\n",
    "              gamma=0, model_ckhpt_path='model_xgboost.json'):\n",
    "    \n",
    "    # Initialize XGB with objective function\n",
    "    parameters = {\"objective\": objective,\n",
    "              \"n_estimators\": 200,\n",
    "              \"eta\": learning_rate,\n",
    "              \"lambda\": lambda_,\n",
    "              \"gamma\": gamma,\n",
    "              \"max_depth\": None,\n",
    "              \"min_child_weight\": min_child_weight,\n",
    "              \"verbosity\": 0}\n",
    "\n",
    "    \n",
    "    model = xgb.XGBRegressor(**parameters)\n",
    "    model.fit(X_train, y_train)\n",
    "    model.save_model(model_ckhpt_path)\n",
    "     \n",
    "    # generate predictions\n",
    "    y_pred_train = model.predict(X_train).reshape(-1,1)\n",
    "    y_pred = model.predict(X_val).reshape(-1,1)\n",
    "    \n",
    "    # calculate errors\n",
    "    rmse_train = mean_squared_error(y_pred_train, y_train, squared=False)\n",
    "    rmse_val = mean_squared_error(y_pred, y_val, squared=False)\n",
    "    print(f\"rmse training: {rmse_train:.3f}\\t rmse validation: {rmse_val:.3f}\")\n",
    "    __compute_rel_predictions__(y_pred, y_val)\n",
    "    __compute_mae__(y_pred, y_val)\n",
    "    \n",
    "    # plot results\n",
    "    y_train = np.array(y_train).reshape(-1,1)\n",
    "    y_val = np.array(y_val).reshape(-1,1)\n",
    "    \n",
    "    # y_train_dummy = y_train\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "    axes[0].scatter(y_pred_train, y_train, alpha=0.5, s=10)\n",
    "    axes[0].plot(y_train, y_train, 'r')\n",
    "    axes[0].set_xlabel('predicted values')\n",
    "    axes[0].set_ylabel('true values')\n",
    "    axes[0].set_title(f\"Training, rmse: {rmse_train:.3f}\")\n",
    "    axes[1].scatter(y_pred, y_val, alpha=0.5, s=10)\n",
    "    axes[1].plot(y_val, y_val, 'r')\n",
    "    axes[1].set_xlabel('predicted values')\n",
    "    axes[1].set_ylabel('true values')\n",
    "    axes[1].set_title(f\"Validation, rmse: {rmse_val:.3f}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "    frequency, bins = np.histogram(y_train, bins=50, range=[np.min(y_pred_train), np.max(y_pred_train)])\n",
    "    axes[0].hist(y_train, alpha=0.5, bins=bins, density='true', label=\"train\")\n",
    "    axes[0].hist(y_pred_train, alpha=0.5, bins=bins, density='true', label=\"predictions\")\n",
    "    axes[0].set_xlabel('Label Scale (0-10)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f\"Histogram of label distribution\")\n",
    "    axes[0].legend()\n",
    "    axes[1].hist(y_val, alpha=0.5, bins=bins, density='true', label=\"validation\")\n",
    "    axes[1].hist(y_pred, alpha=0.5, bins=bins, density='true', label=\"prediction\")\n",
    "    axes[1].set_xlabel('Label Scale (0-10)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f\"Histogram of label distribution\")\n",
    "    axes[1].legend()\n",
    "    return y_pred_train, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assym_loss_over_pred(y_val, y_pred):\n",
    "    grad = np.where((y_val - y_pred)>0, -2.0*2.0*(y_val - y_pred), -2.0*(y_val - y_pred))\n",
    "    hess = np.where((y_val - y_pred)>0, 2.0*2.0, 2.0)\n",
    "    return grad, hess\n",
    "\n",
    "def assym_loss_under_pred(y_val, y_pred):\n",
    "    grad = np.where((y_val - y_pred)<0, -2.0*(y_val - y_pred), -2.0*2.0*(y_val - y_pred))\n",
    "    hess = np.where((y_val - y_pred)<0, 2.0, 2.0*2.0)\n",
    "    return grad, hess    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __compute_mbe__(y_pred, y_test):\n",
    "    mbe = np.zeros(len(y_test))\n",
    "    for i in range(0, len(y_test)) :\n",
    "        mbe[i] = y_pred[i] - y_test[i]\n",
    "    print('MBE = ', np.mean(mbe))    \n",
    "    return mbe   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __compute_rel_predictions__(y_pred, y_test) :\n",
    "    error = np.zeros(len(y_test))\n",
    "    neg_error = 0\n",
    "    pos_error = 0\n",
    "    for i in range(0, len(y_test)) :\n",
    "        error[i] = y_pred[i] - y_test[i]\n",
    "        if error[i] < 0 :\n",
    "            neg_error += 1 ## Under prediction --> more dangerous\n",
    "        elif error[i] > 0 :\n",
    "            pos_error += 1 ## Over prediction\n",
    "    print('Underpredictions :', neg_error/len(y_test)) \n",
    "    print('Overpredictions :', pos_error/len(y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinexLoss_over_pred(y_predicted, y_true):\n",
    "    a = -0.5\n",
    "    E = y_true - y_predicted\n",
    "    grad = (2/a)*(np.exp(a*E) - 1)## negative of first derivative || Remember you are taking the derivative wrt y_pred\n",
    "    hess = 2*(np.exp(a*E))\n",
    "    # grad = 4*E**3\n",
    "    # hess = 12*E**2\n",
    "    return grad, hess\n",
    "\n",
    "def LinexLoss_under_pred(y_predicted, y_true):\n",
    "    a = 1\n",
    "    E = y_true - y_predicted\n",
    "    grad = (2/a)*(np.exp(a*E) - 1)## negative of first derivative || Remember you are taking the derivative wrt y_pred\n",
    "    hess = 2*(np.exp(a*E))\n",
    "    # grad = 4*E**3\n",
    "    # hess = 12*E**2\n",
    "    return grad, hess    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_pred_over_assym = xgb_model(X_train, Y_train, X_test, Y_test, objective=assym_loss_over_pred, learning_rate=0.01, gamma=10)\n",
    "## higher the gamma --> more regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_pred_over_assym = xgb_model(X_train, Y_train, X_test, Y_test, objective=LinexLoss_over_pred, learning_rate=0.01, gamma=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ziptie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_zp, X_test_zp, Y_train_zp, Y_test_zp = sklearn.model_selection.train_test_split(\n",
    "    df_scaled_ziptie.drop(['final_fatigue', 'init_fatigue'], axis='columns'), ziptie_main_dataframe['final_fatigue'], test_size=0.3)\n",
    "\n",
    "X_train_zp = X_train_zp.to_numpy()\n",
    "Y_train_zp = Y_train_zp.to_numpy()\n",
    "\n",
    "X_test_zp  = X_test_zp.to_numpy()\n",
    "Y_test_zp  = Y_test_zp.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_pred_over_assym_zp = xgb_model(X_train_zp, Y_train_zp, X_test_zp, Y_test_zp, objective=assym_loss_over_pred, learning_rate=0.05, gamma=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_pred_over_linex_zp = xgb_model(X_train_zp, Y_train_zp, X_test_zp, Y_test_zp, objective=LinexLoss_over_pred, learning_rate=0.05, gamma=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combo, X_test_combo, Y_train_combo, Y_test_combo = sklearn.model_selection.train_test_split(\n",
    "    df_scaled_combined.drop(['final_fatigue', 'init_fatigue'], axis='columns'), df_scaled_combined['final_fatigue'], test_size=0.3)\n",
    "\n",
    "X_train_combo = X_train_combo.to_numpy()\n",
    "Y_train_combo = Y_train_combo.to_numpy()\n",
    "\n",
    "X_test_combo  = X_test_combo.to_numpy()\n",
    "Y_test_combo  = Y_test_combo.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(121212)\n",
    "_, y_pred_over_linex_combo = xgb_model(X_train_combo, Y_train_combo, X_test_combo, Y_test_combo, objective=LinexLoss_over_pred, learning_rate=0.01, gamma=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2711)\n",
    "_, y_pred_over_assym_combo = xgb_model(X_train_combo, Y_train_combo, X_test_combo, Y_test_combo, objective=assym_loss_over_pred, learning_rate=0.05, gamma=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('wav2vec': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "452e40ca39e9a9f253f95083a5a1e40562e1a769518b37328592047353bd5300"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
